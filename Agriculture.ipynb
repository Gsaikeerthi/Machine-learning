{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b7abb9da-6595-4948-a79b-a782982b45dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import xgboost as xgb\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from xgboost import XGBClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9b8fad6a-3aaa-465d-9cda-746a6d4f76d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to categorize features into numeric, categorical, binary and features to drop\n",
    "def categorize_features(df):\n",
    "    numeric_features = []\n",
    "    categorical_features = []\n",
    "    binary_features = []\n",
    "    features_to_drop = ['UID', 'RawLocationId', 'TownId', 'DistrictId', 'FarmingCommunityId',\n",
    "                       'AgriculturalPostalZone', 'ValuationYear', 'TaxOverdueYear',\n",
    "                       'AgricultureZoningCode', 'OtherZoningCode']\n",
    "    #Preddefined categorical features that need special encoding\n",
    "    encoded_categorical_features = [\n",
    "        'TypeOfIrrigationSystem', 'CropFieldConfiguration', 'FarmClassification',\n",
    "        'HarvestProcessingType', 'LandUsageType', 'FieldZoneLevel', 'FieldConstructionType'\n",
    "    ]\n",
    "\n",
    "    #Classifying features based on datatypes and unique values\n",
    "    for column in df.columns:\n",
    "        # Skip features that we know we wanted to drop\n",
    "        if column in features_to_drop:\n",
    "            continue\n",
    "            \n",
    "        # To get number of unique values and data type\n",
    "        n_unique = df[column].nunique()\n",
    "        dtype = df[column].dtype\n",
    "\n",
    "        if column in encoded_categorical_features:\n",
    "            categorical_features.append(column)\n",
    "            continue\n",
    "    \n",
    "        # Numerical features\n",
    "        if dtype in ['int64', 'float64']:\n",
    "            if n_unique <= 2:  # If unique values are 0,1 or 2 then represent them as binary\n",
    "                binary_features.append(column)\n",
    "            else:\n",
    "                numeric_features.append(column)\n",
    "                \n",
    "        # Categorical features\n",
    "        elif dtype == 'object' or dtype == 'category':\n",
    "            categorical_features.append(column)\n",
    "\n",
    "    return numeric_features, categorical_features, binary_features, features_to_drop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a1225f46-edc0-4646-b2f0-9b10a9af7d13",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function to preprocess training and test datasets\n",
    "def preprocess_data(df, test_df):\n",
    "    \n",
    "    # Calculating the missing percentages\n",
    "    missing_percentage = df.isnull().mean() * 100\n",
    "    \n",
    "    # Identifying columns with more than 90% missing values\n",
    "    columns_to_drop = missing_percentage[missing_percentage > 90].index\n",
    "    #print(\"Columns dropped due to more than 90% missing values:\")\n",
    "    #print(columns_to_drop)\n",
    "    \n",
    "    # Dropping the colums that satisfy above condition\n",
    "    df = df.drop(columns=columns_to_drop, axis=1)\n",
    "    test_df = test_df.drop(columns=columns_to_drop, axis=1)\n",
    "    \n",
    "    # Recalculating the missing percentages for the remaining columns\n",
    "    remaining_missing_percentage = df.isnull().mean() * 100\n",
    "    column_info = pd.DataFrame({\n",
    "        'Missing_Percentage': remaining_missing_percentage,\n",
    "        'Data_Type': df.dtypes\n",
    "    })\n",
    "    #print(\"\\nMissing percentages and data types for remaining columns:\")\n",
    "    #print(column_info)\n",
    "\n",
    "    \n",
    "    # Fill columns with a single unique value and NaN with 0 #\n",
    "    unique_value_columns = []\n",
    "    for col in df.columns:\n",
    "        unique_values = df[col].dropna().unique()\n",
    "        if len(unique_values) == 1 and df[col].isnull().any():\n",
    "            unique_value = unique_values[0]\n",
    "            unique_value_columns.append((col, unique_value))\n",
    "    \n",
    "    for col, unique_value in unique_value_columns:\n",
    "        df[col] = df[col].fillna(0)\n",
    "        test_df[col] = test_df[col].fillna(0)\n",
    "    \n",
    "    #print(\"Filled missing values in columns with a single unique value with 0.\")\n",
    "    \n",
    "\n",
    "    #Categorising the features to apply imputers\n",
    "    numeric_features, categorical_features, binary_features, features_to_drop = categorize_features(df)\n",
    "\n",
    "    #print(\"Numeric features:\", len(numeric_features))\n",
    "    #print(\"Categorical features:\", len(categorical_features))\n",
    "    #print(\"Binary features:\", len(binary_features))\n",
    "    #print(\"Features to drop:\", len(features_to_drop))\n",
    "\n",
    "    #Handling missing values and scaling numeric features\n",
    "    #Note: Changes made in train data-set are also meant to be done in test data-set\n",
    "    for col in numeric_features:\n",
    "        if col in df.columns:\n",
    "            # Calculate median from training data\n",
    "            median_val = df[col].median()\n",
    "            # Fill missing values in both training and test\n",
    "            df[col] = df[col].fillna(median_val)\n",
    "            test_df[col] = test_df[col].fillna(median_val)\n",
    "            \n",
    "            # Scale the features\n",
    "            mean_val = df[col].mean()\n",
    "            std_val = df[col].std()\n",
    "            if std_val != 0:\n",
    "                df[col] = (df[col] - mean_val) / std_val\n",
    "                test_df[col] = (test_df[col] - mean_val) / std_val\n",
    "\n",
    "    #Encoding categorical values\n",
    "    for col in categorical_features:\n",
    "        if col in df.columns:\n",
    "            # Fill missing values with mode from training data\n",
    "            mode_val = df[col].mode()[0]\n",
    "            df[col] = df[col].fillna(mode_val)\n",
    "            test_df[col] = test_df[col].fillna(mode_val)\n",
    "            # One-hot encoding\n",
    "            dummies_train = pd.get_dummies(df[col], prefix=col, drop_first=True)\n",
    "            dummies_test = pd.get_dummies(test_df[col], prefix=col, drop_first=True)\n",
    "            \n",
    "            # Ensure test has all columns from training\n",
    "            for col_dummy in dummies_train.columns:\n",
    "                if col_dummy not in dummies_test.columns:\n",
    "                    dummies_test[col_dummy] = 0\n",
    "                    \n",
    "            # Add encoded columns\n",
    "            df = pd.concat([df, dummies_train], axis=1)\n",
    "            test_df = pd.concat([test_df, dummies_test], axis=1)\n",
    "            \n",
    "            # Drop original categorical column\n",
    "            df = df.drop(col, axis=1)\n",
    "            test_df = test_df.drop(col, axis=1)\n",
    "            \n",
    "    #Handling binary features \n",
    "    for col in binary_features:\n",
    "        if col in df.columns:\n",
    "            # Fill missing values with mode from training data\n",
    "            mode_val = df[col].mode()[0]\n",
    "            df[col] = df[col].fillna(mode_val)\n",
    "            test_df[col] = test_df[col].fillna(mode_val)\n",
    "            \n",
    "            # Convert to numeric if not already\n",
    "            df[col] = pd.to_numeric(df[col], errors='coerce')\n",
    "            test_df[col] = pd.to_numeric(test_df[col], errors='coerce')\n",
    "\n",
    "    # Drop unnecessary features from the dataset\n",
    "    for col in features_to_drop:\n",
    "        if col in df.columns:\n",
    "            df = df.drop(col, axis=1)\n",
    "        if col in test_df.columns:\n",
    "            test_df = test_df.drop(col, axis=1)\n",
    "            \n",
    "    test_df = test_df[df.columns]\n",
    "    \n",
    "    return df, test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fb0d434e-bb33-47f7-b1e2-0a664d019ca6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "usage: ipykernel_launcher.py [-h] [--train-file TRAIN_FILE] [--test-file TEST_FILE]\n",
      "                             [--predictions-file PREDICTIONS_FILE]\n",
      "ipykernel_launcher.py: error: unrecognized arguments: -f C:\\Users\\saike\\AppData\\Roaming\\jupyter\\runtime\\kernel-61a1ecbf-eed9-479e-b4c9-8574f1635588.json\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "2",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[1;31mSystemExit\u001b[0m\u001b[1;31m:\u001b[0m 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\saike\\AppData\\Roaming\\Python\\Python312\\site-packages\\IPython\\core\\interactiveshell.py:3585: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
      "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "#Main function to handle data loading, preprocessing, training and prediction. \n",
    "def make_predictions(test_fname, predictions_fname):\n",
    "    # Load training and test data\n",
    "    train = pd.read_csv('train.csv')\n",
    "    test = pd.read_csv(test_fname)\n",
    "    \n",
    "    # Store test UIDs\n",
    "    test_uids = test['UID'].copy()\n",
    "    \n",
    "    # Drop rows with missing target in training data\n",
    "    train = train.dropna(subset=['Target'])\n",
    "    \n",
    "    # Split features and target\n",
    "    Y_train = train['Target']\n",
    "    X_train = train.drop(columns=['Target'])\n",
    "    \n",
    "    # Preprocess data\n",
    "    X_train, test = preprocess_data(X_train, test)\n",
    "    \n",
    "    # Encode target labels\n",
    "    le = LabelEncoder()\n",
    "    Y_train = le.fit_transform(Y_train)\n",
    "    \n",
    "    # Compute class weights\n",
    "    class_weights = compute_class_weight('balanced', classes=np.unique(Y_train), y=Y_train)\n",
    "    class_weight_dict = {label: weight for label, weight in zip(np.unique(Y_train), class_weights)}\n",
    "    weights = np.array([class_weight_dict[label] for label in Y_train])\n",
    "    \n",
    "    # Train model\n",
    "    model = XGBClassifier(\n",
    "        learning_rate=0.01,\n",
    "        n_estimators=500,\n",
    "        max_depth=8,\n",
    "        min_child_weight=1,\n",
    "        gamma=0,\n",
    "        subsample=0.8,\n",
    "        colsample_bytree=0.8,\n",
    "        objective='multi:softmax',\n",
    "        random_state=42\n",
    "    )\n",
    "    \n",
    "    model.fit(X_train, Y_train, sample_weight=weights)\n",
    "    \n",
    "    # Make predictions on the test dataset\n",
    "    predictions = model.predict(test)\n",
    "    prediction_labels = le.inverse_transform(predictions)\n",
    "    # Saving those predictions\n",
    "    results_df = pd.DataFrame({\n",
    "        'UID': test_uids,\n",
    "        'Target': prediction_labels\n",
    "    })\n",
    "    results_df.to_csv(predictions_fname, index=False)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument(\"--train-file\", type=str, help='file path of train.csv')\n",
    "    parser.add_argument(\"--test-file\", type=str, help='file path of test.csv')\n",
    "    parser.add_argument(\"--predictions-file\", type=str, help='save path of predictions')\n",
    "    args = parser.parse_args()\n",
    "    make_predictions(args.test_file, args.predictions_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc4e0365-4d9b-4520-afb9-f2ac35a8564c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
